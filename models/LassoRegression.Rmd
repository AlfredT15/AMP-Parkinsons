```{r}
#install.packages("coefplot")
library(dplyr)
library(glmnet)
library(caret)
# library(coefplot)
```


```{r}
month_36 <- read.csv("/home/alfred/Code/Kaggle/AMP-Parkinsons/data/grouped_by_month_prot/month_36.csv")
head(month_36)

```


```{r}
month_36 <- subset(month_36, select = -c(visit_month, visit_id, upd23b_medication_Off, upd23b_medication_On))
head(month_36)
```


```{r}
# Define the target variables
targets <- c("updrs_1", "updrs_2", "updrs_3", "updrs_4")

# Loop through the target variables
for (target in targets) {
  # Split data into training and test sets
  set.seed(123)
  train_index <- sample(nrow(month_36), 0.7 * nrow(month_36))
  train_data <- month_36[train_index, ]
  test_data <- month_36[-train_index, ]
  
  # Define the predictors and response variable
  predictors <- names(month_36)[!names(month_36) %in% c(targets)]
  response <- target
  
  # Perform Lasso regression
  lasso_model <- cv.glmnet(as.matrix(train_data[, predictors]), train_data[, response],
                            alpha = 1, nfolds = 5)
  
  # Find the optimal lambda value
  optimal_lambda <- lasso_model$lambda.min
  
  # Fit the model using the optimal lambda value
  lasso_fit <- glmnet(as.matrix(train_data[, predictors]), train_data[, response], alpha = 1,
                      lambda = optimal_lambda)
  
  # Make predictions on test data
  lasso_predictions <- predict(lasso_fit, newx = as.matrix(test_data[, predictors]))
  
  # Calculate the MAPE+1 error
  mape_error <- mean(abs(test_data[, response] - lasso_predictions)/(abs(test_data[, response])+1))
  
  # Print the MAPE+1 error
  print(paste0("MAPE+1 error for ", target, " Lasso regression: ", mape_error))
  
  # Extract non-zero coefficients for the optimal lambda
  lasso_coef <- coef(lasso_fit)
  important_predictors <- row.names(lasso_coef)[which(lasso_coef[,1] != 0)]
  
  # Print the important variables
  cat("At Most 5 Important variables for", target, ": ")
  for (i in 1:min(c(5,length(important_predictors)))) {
    coef_val <- lasso_coef[rownames(lasso_coef) == important_predictors[i], 1]
    cat(paste(important_predictors[i], "(", coef_val, "), ", sep = ""))
  }
  cat("\n")
  cat("\n")
  
  plot(test_data[, response], lasso_predictions, main = paste0("Actual vs. predicted for ", target), xlab = "Actual values", ylab = "Predicted values")
  abline(0, 1, col = "red")
  
  residuals <- test_data[, response] - lasso_predictions
  plot(lasso_predictions, residuals, main = paste0("Residual plot for ", target),xlab = "Predicted values", ylab = "Residuals")
  abline(0, 0, col = "red")
  
  plot(lasso_model, main = paste0("Mean Squared Error for ", target))
  
  # Make predictions on training data
  lasso_train_predictions <- predict(lasso_fit, newx = as.matrix(train_data[, predictors]))
  
  # Calculate R-squared and RMSE for training data
  train_r_squared <- cor(train_data[, response], lasso_train_predictions)^2
  train_rmse <- sqrt(mean((train_data[, response] - lasso_train_predictions)^2))
  
  # Make predictions on test data
  lasso_test_predictions <- predict(lasso_fit, newx = as.matrix(test_data[, predictors]))
  
  # Calculate R-squared and RMSE for test data
  test_r_squared <- cor(test_data[, response], lasso_test_predictions)^2
  test_rmse <- sqrt(mean((test_data[, response] - lasso_test_predictions)^2))
  
  # Calculate the AIC and BIC
  tLL <- ridge_fit$nulldev - deviance(ridge_fit)
  k <- ridge_fit$df
  n <- ridge_fit$nobs
  AIC <- -tLL+2*k+2*k*(k+1)/(n-k-1)

  BIC<-log(n)*k - tLL
  
  # Print the results
  cat(paste0("Results for ", target, " Lasso regression:\n"))
  cat(paste0("Training R-squared: ", round(train_r_squared, 3), "\n"))
  cat(paste0("Training RMSE: ", round(train_rmse, 3), "\n"))
  cat(paste0("Test R-squared: ", round(test_r_squared, 3), "\n"))
  cat(paste0("Test RMSE: ", round(test_rmse, 3), "\n"))
  cat(paste0("AIC: ", round(AIC, 3), "\n"))
  cat(paste0("BIC: ", round(BIC, 3), "\n\n"))
}
```


